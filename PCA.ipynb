{"cells":[{"cell_type":"markdown","source":["### Visualization : Two-dimensional Gaussians"],"metadata":{}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nimport numpy as np\n\ndef prepare_plot(xticks, yticks, figsize=(10.5, 6), hide_labels=False, grid_color='#999999',\n                 grid_width=1.0):\n    \"\"\"Template for generating the plot layout.\"\"\"\n    plt.close()\n    fig, ax = plt.subplots(figsize=figsize, facecolor='white', edgecolor='white')\n    ax.axes.tick_params(labelcolor='#999999', labelsize='10')\n    for axis, ticks in [(ax.get_xaxis(), xticks), (ax.get_yaxis(), yticks)]:\n        axis.set_ticks_position('none')\n        axis.set_ticks(ticks)\n        axis.label.set_color('#999999')\n        if hide_labels: axis.set_ticklabels([])\n    plt.grid(color=grid_color, linewidth=grid_width, linestyle='-')\n    map(lambda position: ax.spines[position].set_visible(False), ['bottom', 'top', 'left', 'right'])\n    return fig, ax\n\ndef create_2D_gaussian(mn, variance, cov, n):\n    \"\"\"Randomly sample points from a two-dimensional Gaussian distribution\"\"\"\n    np.random.seed(142)\n    return np.random.multivariate_normal(np.array([mn, mn]), np.array([[variance, cov], [cov, variance]]), n)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["data_random = create_2D_gaussian(mn=50, variance=1, cov=0, n=100)\n\n# generate layout and plot data\nfig, ax = prepare_plot(np.arange(46, 55, 2), np.arange(46, 55, 2))\nax.set_xlabel(r'Simulated $x_1$ values'), ax.set_ylabel(r'Simulated $x_2$ values')\nax.set_xlim(45, 54.5), ax.set_ylim(45, 54.5)\nplt.scatter(data_random[:,0], data_random[:,1], s=14**2, c='#d6ebf2', edgecolors='#8cbfd0', alpha=0.75)\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["data_correlated = create_2D_gaussian(mn=50, variance=1, cov=.9, n=100)\n\n# generate layout and plot data\nfig, ax = prepare_plot(np.arange(46, 55, 2), np.arange(46, 55, 2))\nax.set_xlabel(r'Simulated $x_1$ values'), ax.set_ylabel(r'Simulated $x_2$ values')\nax.set_xlim(45.5, 54.5), ax.set_ylim(45.5, 54.5)\nplt.scatter(data_correlated[:,0], data_correlated[:,1], s=14**2, c='#d6ebf2',\n            edgecolors='#8cbfd0', alpha=0.75)\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["###  Interpreting PCA"],"metadata":{}},{"cell_type":"code","source":["correlated_data = sc.parallelize(data_correlated)\n\nmean_correlated = correlated_data.sum()/100\ncorrelated_data_zero_mean = correlated_data.map(lambda x:x-mean_correlated)\n\nprint mean_correlated\nprint correlated_data.take(1)\nprint correlated_data_zero_mean.take(1)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["### Covariance Function"],"metadata":{}},{"cell_type":"code","source":["def estimate_covariance(data):\n    \"\"\"Compute the covariance matrix for a given rdd.\n\n    Note:\n        The multi-dimensional covariance array should be calculated using outer products.  Don't\n        forget to normalize the data by first subtracting the mean.\n\n    Args:\n        data (RDD of np.ndarray):  An `RDD` consisting of NumPy arrays.\n\n    Returns:\n        np.ndarray: A multi-dimensional array where the number of rows and columns both equal the\n            length of the arrays in the input `RDD`.\n    \"\"\"\n    num_data=data.count()\n    m=data.sum()/num_data\n    return (data.map(lambda x:x-m).map(lambda x:np.outer(x,np.transpose(x))).sum())/num_data\n\ncorrelated_cov_auto= estimate_covariance(correlated_data)\nprint correlated_cov_auto"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["###  Eigendecomposition"],"metadata":{}},{"cell_type":"code","source":["from numpy.linalg import eigh\n\n# Calculate the eigenvalues and eigenvectors from correlated_cov_auto\neig_vals, eig_vecs = eigh(correlated_cov_auto)\nprint 'eigenvalues: {0}'.format(eig_vals)\nprint '\\neigenvectors: \\n{0}'.format(eig_vecs)\n\n# Use np.argsort to find the top eigenvector based on the largest eigenvalue\ninds = np.argsort(eig_vals)\ntop_component = eig_vecs[inds[1]]\nprint '\\ntop principal component: {0}'.format(top_component)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["###  PCA scores"],"metadata":{}},{"cell_type":"code","source":["correlated_data_scores = correlated_data.map(lambda x:np.dot(x,top_component))\nprint 'one-dimensional data (first three):\\n{0}'.format(np.asarray(correlated_data_scores.take(3)))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["### PCA function"],"metadata":{}},{"cell_type":"code","source":["def pca(data, k=2):\n    \"\"\"Computes the top `k` principal components, corresponding scores, and all eigenvalues.\n\n    Note:\n        All eigenvalues should be returned in sorted order (largest to smallest). `eigh` returns\n        each eigenvectors as a column.  This function should also return eigenvectors as columns.\n\n    Args:\n        data (RDD of np.ndarray): An `RDD` consisting of NumPy arrays.\n        k (int): The number of principal components to return.\n\n    Returns:\n        tuple of (np.ndarray, RDD of np.ndarray, np.ndarray): A tuple of (eigenvectors, `RDD` of\n            scores, eigenvalues).  Eigenvectors is a multi-dimensional array where the number of\n            rows equals the length of the arrays in the input `RDD` and the number of columns equals\n            `k`.  The `RDD` of scores has the same number of rows as `data` and consists of arrays\n            of length `k`.  Eigenvalues is an array of length d (the number of features).\n    \"\"\"\n    cov = estimate_covariance(data)\n    eigvals, eigvecs = eigh(cov)\n    ind = np.argsort(eigvals)[::-1]\n    k_comp = eigvecs[:,ind[:k]]\n    score = data.map(lambda x:x.dot(k_comp))\n    # Return the `k` principal components, `k` scores, and all eigenvalues\n    return (k_comp,score,eigvals[ind])\n\n# Run pca on correlated_data with k = 2\ntop_components_correlated, correlated_data_scores_auto, eigenvalues_correlated = pca(correlated_data)\n\n# Note that the 1st principal component is in the first column\nprint 'top_components_correlated: \\n{0}'.format(top_components_correlated)\nprint ('\\ncorrelated_data_scores_auto (first three): \\n{0}'\n       .format('\\n'.join(map(str, correlated_data_scores_auto.take(3)))))\nprint '\\neigenvalues_correlated: \\n{0}'.format(eigenvalues_correlated)\n\n# Create a higher dimensional test set\npca_test_data = sc.parallelize([np.arange(x, x + 4) for x in np.arange(0, 20, 4)])\ncomponents_test, test_scores, eigenvalues_test = pca(pca_test_data, 3)\n\nprint '\\npca_test_data: \\n{0}'.format(np.array(pca_test_data.collect()))\nprint '\\ncomponents_test: \\n{0}'.format(components_test)\nprint ('\\ntest_scores (first three): \\n{0}'\n       .format('\\n'.join(map(str, test_scores.take(3)))))\nprint '\\neigenvalues_test: \\n{0}'.format(eigenvalues_test)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["### (2b) PCA on `data_random`\n\nNext, use the PCA function we just developed to find the top two principal components of the spherical `data_random` we created in Visualization 1.\n\nFirst, we need to convert `data_random` to the RDD `random_data_rdd`, and do all subsequent operations on `random_data_rdd`."],"metadata":{}},{"cell_type":"markdown","source":["### Visualization : PCA projection"],"metadata":{}},{"cell_type":"code","source":["def project_points_and_get_lines(data, components, x_range):\n    \"\"\"Project original data onto first component and get line details for top two components.\"\"\"\n    top_component = components[:, 0]\n    slope1, slope2 = components[1, :2] / components[0, :2]\n\n    means = data.mean()[:2]\n    demeaned = data.map(lambda v: v - means)\n    projected = demeaned.map(lambda v: (v.dot(top_component) /\n                                        top_component.dot(top_component)) * top_component)\n    remeaned = projected.map(lambda v: v + means)\n    x1,x2 = zip(*remeaned.collect())\n\n    line_start_P1_X1, line_start_P1_X2 = means - np.asarray([x_range, x_range * slope1])\n    line_end_P1_X1, line_end_P1_X2 = means + np.asarray([x_range, x_range * slope1])\n    line_start_P2_X1, line_start_P2_X2 = means - np.asarray([x_range, x_range * slope2])\n    line_end_P2_X1, line_end_P2_X2 = means + np.asarray([x_range, x_range * slope2])\n\n    return ((x1, x2), ([line_start_P1_X1, line_end_P1_X1], [line_start_P1_X2, line_end_P1_X2]),\n            ([line_start_P2_X1, line_end_P2_X1], [line_start_P2_X2, line_end_P2_X2]))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["((x1, x2), (line1X1, line1X2), (line2X1, line2X2)) = \\\n    project_points_and_get_lines(correlated_data, top_components_correlated, 5)\n\n# generate layout and plot data\nfig, ax = prepare_plot(np.arange(46, 55, 2), np.arange(46, 55, 2), figsize=(7, 7))\nax.set_xlabel(r'Simulated $x_1$ values'), ax.set_ylabel(r'Simulated $x_2$ values')\nax.set_xlim(45.5, 54.5), ax.set_ylim(45.5, 54.5)\nplt.plot(line1X1, line1X2, linewidth=3.0, c='#8cbfd0', linestyle='--')\nplt.plot(line2X1, line2X2, linewidth=3.0, c='#d6ebf2', linestyle='--')\nplt.scatter(data_correlated[:,0], data_correlated[:,1], s=14**2, c='#d6ebf2',\n            edgecolors='#8cbfd0', alpha=0.75)\nplt.scatter(x1, x2, s=14**2, c='#62c162', alpha=.75)\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["((x1, x2), (line1X1, line1X2), (line2X1, line2X2)) = \\\n    project_points_and_get_lines(random_data_rdd, top_components_random, 5)\n\n# generate layout and plot data\nfig, ax = prepare_plot(np.arange(46, 55, 2), np.arange(46, 55, 2), figsize=(7, 7))\nax.set_xlabel(r'Simulated $x_1$ values'), ax.set_ylabel(r'Simulated $x_2$ values')\nax.set_xlim(45.5, 54.5), ax.set_ylim(45.5, 54.5)\nplt.plot(line1X1, line1X2, linewidth=3.0, c='#8cbfd0', linestyle='--')\nplt.plot(line2X1, line2X2, linewidth=3.0, c='#d6ebf2', linestyle='--')\nplt.scatter(data_random[:,0], data_random[:,1], s=14**2, c='#d6ebf2',\n            edgecolors='#8cbfd0', alpha=0.75)\nplt.scatter(x1, x2, s=14**2, c='#62c162', alpha=.75)\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["### Visualization : Three-dimensional data"],"metadata":{}},{"cell_type":"code","source":["from mpl_toolkits.mplot3d import Axes3D\n\nm = 100\nmu = np.array([50, 50, 50])\nr1_2 = 0.9\nr1_3 = 0.7\nr2_3 = 0.1\nsigma1 = 5\nsigma2 = 20\nsigma3 = 20\nc = np.array([[sigma1 ** 2, r1_2 * sigma1 * sigma2, r1_3 * sigma1 * sigma3],\n             [r1_2 * sigma1 * sigma2, sigma2 ** 2, r2_3 * sigma2 * sigma3],\n             [r1_3 * sigma1 * sigma3, r2_3 * sigma2 * sigma3, sigma3 ** 2]])\nnp.random.seed(142)\ndata_threeD = np.random.multivariate_normal(mu, c, m)\n\nfrom matplotlib.colors import ListedColormap, Normalize\nfrom matplotlib.cm import get_cmap\nnorm = Normalize()\ncmap = get_cmap(\"Blues\")\nclrs = cmap(np.array(norm(data_threeD[:,2])))[:,0:3]\n\nfig = plt.figure(figsize=(11, 6))\nax = fig.add_subplot(121, projection='3d')\nax.azim=-100\nax.scatter(data_threeD[:,0], data_threeD[:,1], data_threeD[:,2], c=clrs, s=14**2)\n\nxx, yy = np.meshgrid(np.arange(-15, 10, 1), np.arange(-50, 30, 1))\nnormal = np.array([0.96981815, -0.188338, -0.15485978])\nz = (-normal[0] * xx - normal[1] * yy) * 1. / normal[2]\nxx = xx + 50\nyy = yy + 50\nz = z + 50\n\nax.set_zlim((-20, 120)), ax.set_ylim((-20, 100)), ax.set_xlim((30, 75))\nax.plot_surface(xx, yy, z, alpha=.10)\n\nax = fig.add_subplot(122, projection='3d')\nax.azim=10\nax.elev=20\n#ax.dist=8\nax.scatter(data_threeD[:,0], data_threeD[:,1], data_threeD[:,2], c=clrs, s=14**2)\n\nax.set_zlim((-20, 120)), ax.set_ylim((-20, 100)), ax.set_xlim((30, 75))\nax.plot_surface(xx, yy, z, alpha=.1)\nplt.tight_layout()\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\nthreeD_data = sc.parallelize(data_threeD)\ncomponents_threeD, threeD_scores, eigenvalues_threeD = pca(threeD_data)\n\nprint 'components_threeD: \\n{0}'.format(components_threeD)\nprint ('\\nthreeD_scores (first three): \\n{0}'\n       .format('\\n'.join(map(str, threeD_scores.take(3)))))\nprint '\\neigenvalues_threeD: \\n{0}'.format(eigenvalues_threeD)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["### Visualization : 2D representation of 3D data"],"metadata":{}},{"cell_type":"code","source":["scores_threeD = np.asarray(threeD_scores.collect())\n\n# generate layout and plot data\nfig, ax = prepare_plot(np.arange(20, 150, 20), np.arange(-40, 110, 20))\nax.set_xlabel(r'New $x_1$ values'), ax.set_ylabel(r'New $x_2$ values')\nax.set_xlim(5, 150), ax.set_ylim(-45, 50)\nplt.scatter(scores_threeD[:, 0], scores_threeD[:, 1], s=14 ** 2, c=clrs, edgecolors='#8cbfd0', alpha=0.75)\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["def variance_explained(data, k=1):\n    \"\"\"Calculate the fraction of variance explained by the top `k` eigenvectors.\n\n    Args:\n        data (RDD of np.ndarray): An RDD that contains NumPy arrays which store the\n            features for an observation.\n        k: The number of principal components to consider.\n\n    Returns:\n        float: A number between 0 and 1 representing the percentage of variance explained\n            by the top `k` eigenvectors.\n    \"\"\"\n    components, scores, eigenvalues = pca(data,k)\n    l=0\n    z=0\n    for i in eigenvalues[:k]:\n      l+= i\n    for s in eigenvalues[:]:\n      z+= s \n    print eigenvalues\n    return l/z\n\nvariance_random_1 = variance_explained(random_data_rdd, 1)\nvariance_correlated_1 = variance_explained(correlated_data, 1)\nvariance_random_2 = variance_explained(random_data_rdd, 2)\nvariance_correlated_2 = variance_explained(correlated_data, 2)\nvariance_threeD_2 = variance_explained(threeD_data, 2)\nprint ('Percentage of variance explained by the first component of random_data_rdd: {0:.1f}%'\n       .format(variance_random_1 * 100))\nprint ('Percentage of variance explained by both components of random_data_rdd: {0:.1f}%'\n       .format(variance_random_2 * 100))\nprint ('\\nPercentage of variance explained by the first component of correlated_data: {0:.1f}%'.\n       format(variance_correlated_1 * 100))\nprint ('Percentage of variance explained by both components of correlated_data: {0:.1f}%'\n       .format(variance_correlated_2 * 100))\nprint ('\\nPercentage of variance explained by the first two components of threeD_data: {0:.1f}%'\n       .format(variance_threeD_2 * 100))"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["## Part 3:  Parsing, inspecting, and preprocessing neuroscience data then perform PCA"],"metadata":{}},{"cell_type":"code","source":["import os\ninput_file = os.path.join('databricks-datasets', 'cs190', 'data-001', 'neuro.txt')\n\nlines = sc.textFile(input_file)\nprint lines.first()[0:100]"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["def parse(line):\n    \"\"\"Parse the raw data into a (`tuple`, `np.ndarray`) pair.\n\n    Note:\n        You should store the pixel coordinates as a tuple of two ints and the elements of the pixel intensity\n        time series as an np.ndarray of floats.\n\n    Args:\n        line (str): A string representing an observation.  Elements are separated by spaces.  The\n            first two elements represent the coordinates of the pixel, and the rest of the elements\n            represent the pixel intensity over time.\n\n    Returns:\n        tuple of tuple, np.ndarray: A (coordinate, pixel intensity array) `tuple` where coordinate is\n            a `tuple` containing two values and the pixel intensity is stored in an NumPy array\n            which contains 240 values.\n    \"\"\"\n    a=line.split(' ')\n    return ((int(a[0]),int(a[1])),np.array(a[2:],dtype=np.float))\n\nraw_data = lines.map(parse)\nraw_data.cache()\nentry = raw_data.first()\nprint 'Length of movie is {0} seconds'.format(len(entry[1]))\nprint 'Number of pixels in movie is {0:,}'.format(raw_data.count())\nprint ('\\nFirst entry of raw_data (with only the first five values of the NumPy array):\\n({0}, {1})'\n       .format(entry[0], entry[1][:5]))"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["###  Min and max fluorescence values across all pixels."],"metadata":{}},{"cell_type":"code","source":["mx = max(raw_data.map(lambda (x,y):max(y)).collect())\nmn = min(raw_data.map(lambda (x,y):min(y)).collect())\n\nprint mn, mx"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["### Visualization : Pixel intensity"],"metadata":{}},{"cell_type":"code","source":["example = raw_data.filter(lambda (k, v): np.std(v) > 100).values().first()\n\n# generate layout and plot data\nfig, ax = prepare_plot(np.arange(0, 300, 50), np.arange(300, 800, 100))\nax.set_xlabel(r'time'), ax.set_ylabel(r'fluorescence')\nax.set_xlim(-20, 270), ax.set_ylim(270, 730)\nplt.plot(range(len(example)), example, c='#8cbfd0', linewidth='3.0')\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["###  Fractional signal change"],"metadata":{}},{"cell_type":"code","source":["def rescale(ts):\n    \"\"\"Take a np.ndarray and return the standardized array by subtracting and dividing by the mean.\n\n    Note:\n        You should first subtract the mean and then divide by the mean.\n\n    Args:\n        ts (np.ndarray): Time series data (`np.float`) representing pixel intensity.\n\n    Returns:\n        np.ndarray: The times series adjusted by subtracting the mean and dividing by the mean.\n    \"\"\"\n    m = ts.mean()\n    for i in range(len(ts)):\n      ts[i] = (ts[i]-m)/m\n    return ts\n\nscaled_data = raw_data.mapValues(lambda v: rescale(v))\nmn_scaled = scaled_data.map(lambda (k, v): v).map(lambda v: min(v)).min()\nmx_scaled = scaled_data.map(lambda (k, v): v).map(lambda v: max(v)).max()\nprint mn_scaled, mx_scaled"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["### Visualization : Normalized data"],"metadata":{}},{"cell_type":"code","source":["example = scaled_data.filter(lambda (k, v): np.std(v) > 0.1).values().first()\n\n# generate layout and plot data\nfig, ax = prepare_plot(np.arange(0, 300, 50), np.arange(-.1, .6, .1))\nax.set_xlabel(r'time'), ax.set_ylabel(r'fluorescence')\nax.set_xlim(-20, 260), ax.set_ylim(-.12, .52)\nplt.plot(range(len(example)), example, c='#8cbfd0', linewidth='3.0')\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["### PCA on the scaled data"],"metadata":{}},{"cell_type":"code","source":["components_scaled, scaled_scores, eigenvalues_scaled = pca(scaled_data.map(lambda (k,v):v),3)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["### Visualization : Top two components as images"],"metadata":{}},{"cell_type":"code","source":["import matplotlib.cm as cm\n\nscores_scaled = np.vstack(scaled_scores.collect())\nimage_one_scaled = scores_scaled[:, 0].reshape(230, 202).T\n\n# generate layout and plot data\nfig, ax = prepare_plot(np.arange(0, 10, 1), np.arange(0, 10, 1), figsize=(9.0, 7.2), hide_labels=True)\nax.grid(False)\nax.set_title('Top Principal Component', color='#888888')\nimage = plt.imshow(image_one_scaled, interpolation='nearest', aspect='auto', cmap=cm.gray)\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["image_two_scaled = scores_scaled[:, 1].reshape(230, 202).T\n\n# generate layout and plot data\nfig, ax = prepare_plot(np.arange(0, 10, 1), np.arange(0, 10, 1), figsize=(9.0, 7.2), hide_labels=True)\nax.grid(False)\nax.set_title('Second Principal Component', color='#888888')\nimage = plt.imshow(image_two_scaled, interpolation='nearest', aspect='auto', cmap=cm.gray)\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["### Visualization : Top two components as one image"],"metadata":{}},{"cell_type":"code","source":["# Adapted from python-thunder's Colorize.transform where cmap='polar'.\n# Checkout the library at: https://github.com/thunder-project/thunder and\n# http://thunder-project.org/\n\ndef polar_transform(scale, img):\n    \"\"\"Convert points from cartesian to polar coordinates and map to colors.\"\"\"\n    from matplotlib.colors import hsv_to_rgb\n\n    img = np.asarray(img)\n    dims = img.shape\n\n    phi = ((np.arctan2(-img[0], -img[1]) + np.pi/2) % (np.pi*2)) / (2 * np.pi)\n    rho = np.sqrt(img[0]**2 + img[1]**2)\n    saturation = np.ones((dims[1], dims[2]))\n\n    out = hsv_to_rgb(np.dstack((phi, saturation, scale * rho)))\n\n    return np.clip(out * scale, 0, 1)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["# Showing the polar mapping from principal component coordinates to colors.\nx1_abs_max = np.max(np.abs(image_one_scaled))\nx2_abs_max = np.max(np.abs(image_two_scaled))\n\nnum_of_pixels = 300\nx1_vals = np.arange(-x1_abs_max, x1_abs_max, (2 * x1_abs_max) / num_of_pixels)\nx2_vals = np.arange(x2_abs_max, -x2_abs_max, -(2 * x2_abs_max) / num_of_pixels)\nx2_vals.shape = (num_of_pixels, 1)\n\nx1_data = np.tile(x1_vals, (num_of_pixels, 1))\nx2_data = np.tile(x2_vals, (1, num_of_pixels))\n\n\npolar_map = polar_transform(2.0, [x1_data, x2_data])\n\ngrid_range = np.arange(0, num_of_pixels + 25, 25)\nfig, ax = prepare_plot(grid_range, grid_range, figsize=(9.0, 7.2), hide_labels=True)\nimage = plt.imshow(polar_map, interpolation='nearest', aspect='auto')\nax.set_xlabel('Principal component one'), ax.set_ylabel('Principal component two')\ngrid_marks = (2 * grid_range / float(num_of_pixels) - 1.0)\nx1_marks = x1_abs_max * grid_marks\nx2_marks = -x2_abs_max * grid_marks\nax.get_xaxis().set_ticklabels(map(lambda x: '{0:.1f}'.format(x), x1_marks))\nax.get_yaxis().set_ticklabels(map(lambda x: '{0:.1f}'.format(x), x2_marks))\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["brainmap = polar_transform(2.0, [image_one_scaled, image_two_scaled])\n\n# generating layout and plotting data\nfig, ax = prepare_plot(np.arange(0, 10, 1), np.arange(0, 10, 1), figsize=(9.0, 7.2), hide_labels=True)\nax.grid(False)\nimage = plt.imshow(brainmap,interpolation='nearest', aspect='auto')\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":45}],"metadata":{"name":"cs120_lab4_pca","notebookId":4286422718783716},"nbformat":4,"nbformat_minor":0}
